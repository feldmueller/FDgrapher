{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the trained Word Embedding Models\n",
    "META_PATH_WEM = {\n",
    "    \"1990\": \"model/1990.model\",\n",
    "    \"2000\": \"model/2000.model\",\n",
    "    \"2010\": \"model/2010.model\",\n",
    "    \"CDU\": \"model/CDU.model\",\n",
    "    \"SPD\": \"model/SPD.model\",\n",
    "    \"FDP\": \"model/FDP.model\",\n",
    "    \"GRUENE\": \"model/GRUENE.model\",\n",
    "    \"LINKE\": \"model/LINKE.model\",\n",
    "    \"compass\": \"model/compass.model\",\n",
    "}\n",
    "# Provide paths to trained k-Means models or set \"train_new_models\" to True.\n",
    "# Set k to an integer of your choice to specify the number of clustered to be generated. Alternatively, choose a decimal number to orient k according to the number of types/embeddings (k=0.024 corresponds to 2.4 % of the types)\n",
    "KMEANS_PAR = {\n",
    "    \"train_new_models\": True,\n",
    "    \"k\": 0.024,\n",
    "    \"meta_path\": {\n",
    "        \"1990\": \"PATH/model/1990.model.kmeans.pkl\",\n",
    "        \"2000\": \"PATH/model/2000.model.kmeans.pkl\",\n",
    "        \"2010\": \"PATH/model/2010.model.kmeans.pkl\",\n",
    "        \"CDU\": \"PATH/model/CDU.model.kmeans.pkl\",\n",
    "        \"SPD\": \"PATH/model/SPD.model.kmeans.pkl\",\n",
    "        \"FDP\": \"PATH/model/FDP.model.kmeans.pkl\",\n",
    "        \"GRUENE\": \"PATH/model/GRUENE.model.kmeans.pkl\",\n",
    "        \"LINKE\": \"PATH/model/LINKE.model.kmeans.pkl\",\n",
    "        \"compass\": \"PATH/model/compass.model.kmeans.pkl\",\n",
    "    },\n",
    "}\n",
    "# Specify a list of thematic seed words that will be used (and significantly expanded) to determin a thematic centroid as a reference point in the models\n",
    "T_CENTROID_PAR = {\n",
    "    \"seed_words\": [\n",
    "        \"Arbeit\",\n",
    "        \"Arbeiten\",\n",
    "        \"arbeiten\",\n",
    "        \"gearbeitet\",\n",
    "        \"arbeitet\",\n",
    "        \"Tätigkeit\",\n",
    "        \"Tätigkeiten\",\n",
    "        \"Lohn\",\n",
    "        \"Gehalt\",\n",
    "        \"schuften\",\n",
    "        \"geschuftet\",\n",
    "    ],\n",
    "    \"t_sim\": 0.29,\n",
    "    \"neighbors\": 500,\n",
    "    \"cutoff\": 0.05,\n",
    "    \"write_to_disc\": True,\n",
    "}\n",
    "\n",
    "# Path to the vrt file of your CWB corpus\n",
    "CORPUS_VRT_PATH = \"germaparl_decades.vrt\"\n",
    "\n",
    "# in s_attribute_meta you need to map the s_attributes (as specified when importing the vrt file into the CWB) to the metadata used above\n",
    "# you can, optionally, use shorthands for the metadata when labeling the clusters\n",
    "# column specifies which column of the vrt file is the token base layer\n",
    "# if you set keep_layers to True, your CWB corpus will contain both, its old annotations and the cluster annotations\n",
    "# in ignore_meta you can specify values of s_attributes in your CWB corpus that you don't want to add cluster annotations for (or where you don't have clusters)\n",
    "# using replace_meta, you can map s_attribute values to other s_attribute values to treat them as if they were the same when annotating the corpus. For example, {\"a\": \"b\"} would annotate every word of a text with the s_attribute \"a\" with the cluster trained on the \"b\" model.\n",
    "ANNOTATION_PAR = {\n",
    "    \"s_attribute_meta\": {\n",
    "        \"party\": [\n",
    "            \"CDU\",\n",
    "            \"SPD\",\n",
    "            \"FDP\",\n",
    "            \"GRUENE\",\n",
    "            \"LINKE\",\n",
    "        ],\n",
    "        \"decade\": [\"1990\", \"2000\", \"2010\"],\n",
    "    },\n",
    "    \"shorthand\": {\"1990\": \"90\", \"2000\": \"00\", \"2010\": \"10\"},\n",
    "    \"column\": 3,\n",
    "    \"keep_layers\": False,\n",
    "    \"ignore_meta\": {\"parteilos\", \"\"},\n",
    "    \"replace_meta\": {\"CSU\": \"CDU\", \"PDS\": \"LINKE\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to train our models by decade (and by party), we enrich the corpus with this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"PATH/germaparl.vrt\", \"r\", encoding=\"latin1\") as f:\n",
    "    vrt = f.readlines()\n",
    "\n",
    "vrt_out = []\n",
    "for line in vrt:\n",
    "    if line.startswith(\"<year\"):\n",
    "        year = int(line.strip(\"<>\\n\").split(\" \")[1])\n",
    "        if year < 2000:\n",
    "            vrt_out.append(\"<decade 1990>\\n\")\n",
    "        elif year < 2010:\n",
    "            vrt_out.append(\"<decade 2000>\\n\")\n",
    "        elif year >= 2010:\n",
    "            vrt_out.append(\"<decade 2010>\\n\")\n",
    "    vrt_out.append(line)\n",
    "    if line.startswith(\"</year>\"):\n",
    "        vrt_out.append(\"</decade>\\n\")\n",
    "\n",
    "with open(\"germaparl_decades.vrt\", \"w\") as f:\n",
    "    f.write(\"\".join(vrt_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our models, if we don't have any yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"germaparl_decades.vrt\", \"r\") as f:\n",
    "    vrt = f.readlines()\n",
    "\n",
    "meta_sents = {\n",
    "    \"1990\": [],\n",
    "    \"2000\": [],\n",
    "    \"2010\": [],\n",
    "    \"CDU\": [],\n",
    "    \"SPD\": [],\n",
    "    \"FDP\": [],\n",
    "    \"GRUENE\": [],\n",
    "    \"LINKE\": [],\n",
    "    \"\": [],\n",
    "    \"parteilos\": [],\n",
    "    \"all\": [],\n",
    "}\n",
    "# We treat CDU and CSU, PDS and LINKE for the training as if they were the same party\n",
    "for line in vrt:\n",
    "    if line.startswith(\"<party\"):\n",
    "        party = line.strip(\"<>\\n\").split(\" \")[1]\n",
    "        if party == \"CSU\":\n",
    "            party = \"CDU\"\n",
    "        elif party == \"fraktionslos\":\n",
    "            party = \"parteilos\"\n",
    "        elif party == \"PDS\":\n",
    "            party = \"LINKE\"\n",
    "        sent = []\n",
    "    if line.startswith(\"<decade\"):\n",
    "        decade = line.strip(\"<>\\n\").split(\" \")[1]\n",
    "    if \"\\t\" in line:\n",
    "        sent.append(line.strip().split(\"\\t\")[-1])\n",
    "    if \"</year\" in line and (sent[0] != \"(\" and sent[-1] != \")\") and len(sent) > 3:\n",
    "        meta_sents[party].append(sent)\n",
    "        meta_sents[decade].append(sent)\n",
    "        meta_sents[\"all\"].append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick and dirty helper function in order to get frequency dictionaries for every subcorpus (decades and parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_freq_dicts():\n",
    "\n",
    "    meta_dict = {}\n",
    "\n",
    "    for decade in [\"1990\", \"2000\", \"2010\"]:\n",
    "        words = [word for sent in meta_sents[decade] for word in sent]\n",
    "        meta_dict[decade] = Counter(words)\n",
    "\n",
    "    for party in [\"CDU\", \"SPD\", \"FDP\", \"GRUENE\", \"LINKE\", \"\", \"parteilos\"]:\n",
    "        words = [word for sent in meta_sents[party] for word in sent]\n",
    "        meta_dict[party] = Counter(words)\n",
    "\n",
    "    words = [word for sent in meta_sents[\"all\"] for word in sent]\n",
    "    meta_dict[\"all\"] = Counter(words)\n",
    "\n",
    "    return meta_dict\n",
    "\n",
    "\n",
    "meta_freqs = get_freq_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train the TWEC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the compass.\n",
      "Training temporal embeddings: slice model/1990.txt.\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice model/2000.txt.\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice model/2010.txt.\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice model/CDU.txt.\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice model/SPD.txt.\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice model/FDP.txt.\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice model/GRUENE.txt.\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice model/LINKE.txt.\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice model/.txt.\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice model/parteilos.txt.\n",
      "Initializing temporal embeddings from the atemporal compass.\n"
     ]
    }
   ],
   "source": [
    "from twec.twec import TWEC\n",
    "\n",
    "aligner = TWEC(size=200, min_count=1, window=6, workers=10)\n",
    "\n",
    "min_occ = 15\n",
    "path = \"model/compass.txt\"\n",
    "with open(path, \"w\") as f:\n",
    "    for sentence in meta_sents[\"all\"]:\n",
    "        sentence = [word for word in sentence if meta_freqs[\"all\"][word] >= min_occ]\n",
    "        f.write(\" \".join(sentence) + \"\\n\")\n",
    "aligner.train_compass(path, overwrite=True)\n",
    "\n",
    "for meta, sents in meta_sents.items():\n",
    "    if meta == \"all\":\n",
    "        continue\n",
    "    else:\n",
    "        path = f\"model/{meta}.txt\"\n",
    "        with open(path, \"w\") as f:\n",
    "            for sentence in meta_sents[\"all\"]:\n",
    "                sentence = [\n",
    "                    word for word in sentence if meta_freqs[meta][word] >= min_occ\n",
    "                ]\n",
    "                f.write(\" \".join(sentence) + \"\\n\")\n",
    "    aligner.train_slice(path, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, enter the paths to the trained models in the parameter section above.\n",
    "[If you would want to use your own models, you would start here.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "meta_model = {}\n",
    "for meta, path in META_PATH_WEM.items():\n",
    "    meta_model[meta] = Word2Vec.load(path).wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the models have  been loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tätigkeit', 0.6047515869140625),\n",
       " ('Vorarbeit', 0.5334701538085938),\n",
       " ('Arbeiten', 0.49987655878067017),\n",
       " ('Ausbildung', 0.4991936981678009),\n",
       " ('Gesetzgebungsarbeit', 0.4801711440086365),\n",
       " ('Aufklärungsarbeit', 0.4769136309623718),\n",
       " ('Beschäftigung', 0.47630828619003296),\n",
       " ('Zuarbeit', 0.4755493402481079),\n",
       " ('Hilfe', 0.4712493419647217),\n",
       " ('Immobilienaufgaben', 0.464999258518219)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = \"SPD\"\n",
    "meta_model[meta].most_similar(\"Arbeit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weiterarbeiten', 0.6369683742523193),\n",
       " ('leben', 0.5952897071838379),\n",
       " ('gearbeitet', 0.5807672142982483),\n",
       " ('mitarbeiten', 0.5708843469619751),\n",
       " ('arbeitet', 0.5700968503952026),\n",
       " ('mitwirken', 0.5669143795967102),\n",
       " ('forschen', 0.5174206495285034),\n",
       " ('zusammenarbeiten', 0.5101714730262756),\n",
       " ('kämpfen', 0.49115467071533203),\n",
       " ('denken', 0.48813557624816895)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = \"FDP\"\n",
    "meta_model[meta].most_similar(\"arbeiten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Taschengeld', 0.6789494752883911),\n",
       " ('Lohn', 0.6732443571090698),\n",
       " ('Erwerbseinkommen', 0.6674096584320068),\n",
       " ('Honorar', 0.6258935928344727),\n",
       " ('Nettoeinkommen', 0.6141149401664734),\n",
       " ('Wohnort', 0.6094042062759399),\n",
       " ('Lebensalter', 0.6080386638641357),\n",
       " ('Entgelt', 0.6050905585289001),\n",
       " ('Fahrzeug', 0.6004776954650879),\n",
       " ('Einkommen', 0.597220778465271)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = \"GRUENE\"\n",
    "meta_model[meta].most_similar(\"Gehalt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your k-Means clusters will be trained or loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to cluster model 1990: 14:40:34\n",
      "Finished clustering model 1990: 14:40:51\n",
      "Starting to cluster model 2000: 14:40:51\n",
      "Finished clustering model 2000: 14:41:45\n",
      "Starting to cluster model 2010: 14:41:45\n",
      "Finished clustering model 2010: 14:42:29\n",
      "Starting to cluster model CDU: 14:42:29\n",
      "Finished clustering model CDU: 14:43:03\n",
      "Starting to cluster model SPD: 14:43:03\n",
      "Finished clustering model SPD: 14:43:31\n",
      "Starting to cluster model FDP: 14:43:31\n",
      "Finished clustering model FDP: 14:43:40\n",
      "Starting to cluster model GRUENE: 14:43:40\n",
      "Finished clustering model GRUENE: 14:43:52\n",
      "Starting to cluster model LINKE: 14:43:52\n",
      "Finished clustering model LINKE: 14:44:01\n",
      "Starting to cluster model compass: 14:44:01\n",
      "Finished clustering model compass: 14:46:07\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from lib import train_kMeans\n",
    "\n",
    "if KMEANS_PAR[\"train_new_models\"]:\n",
    "    meta_km = train_kMeans(meta_model, META_PATH_WEM, KMEANS_PAR[\"k\"])\n",
    "else:\n",
    "    meta_km = {}\n",
    "    for meta, path in KMEANS_PAR[\"meta_path\"].items():\n",
    "        with open(path, \"rb\") as f:\n",
    "            meta_km[meta] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every cluster will be sorted by closeness to the centroid and named after the three most central words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "\n",
    "meta_word_cluster = {}\n",
    "meta_cluster_words = {}\n",
    "meta_cluster_centroid = {}\n",
    "meta_cluster_label = {}\n",
    "\n",
    "for meta, km in meta_km.items():\n",
    "    model = meta_model[meta]\n",
    "    try:\n",
    "        word_cluster = dict(zip(model.key_to_index.keys(), km.labels_))\n",
    "    except AttributeError:\n",
    "        word_cluster = dict(zip(model.index2word, km.labels_))\n",
    "    cluster_centroid = dict(zip(range(len(km.cluster_centers_)), km.cluster_centers_))\n",
    "    df = pd.DataFrame(word_cluster.items(), columns=[\"word\", \"cluster\"])\n",
    "    df[\"sim\"] = [\n",
    "        1 - distance.cosine(model[word], cluster_centroid[word_cluster[word]])\n",
    "        for word in df[\"word\"]\n",
    "    ]\n",
    "    df = df.sort_values(by=[\"cluster\", \"sim\"], ascending=[True, False])\n",
    "\n",
    "    cluster_words = {}\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        if row[\"cluster\"] in cluster_words:\n",
    "            cluster_words[row[\"cluster\"]].append(row[\"word\"])\n",
    "        else:\n",
    "            cluster_words[row[\"cluster\"]] = [row[\"word\"]]\n",
    "\n",
    "    cluster_label = {}\n",
    "\n",
    "    for cluster, words in cluster_words.items():\n",
    "        cluster_label[cluster] = \"|\".join(words[:3])\n",
    "\n",
    "    meta_word_cluster[meta] = word_cluster\n",
    "    meta_cluster_words[meta] = cluster_words\n",
    "    meta_cluster_centroid[meta] = cluster_centroid\n",
    "    meta_cluster_label[meta] = cluster_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of example clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mitwirken',\n",
       " 'teilzunehmen',\n",
       " 'mitzuwirken',\n",
       " 'teilnehmen',\n",
       " 'gehindert',\n",
       " 'hindern',\n",
       " 'mitarbeiten',\n",
       " 'mitgewirkt',\n",
       " 'mitgearbeitet',\n",
       " 'teilhaben',\n",
       " 'teilgenommen',\n",
       " 'hindert',\n",
       " 'gelegen',\n",
       " 'scheitern',\n",
       " 'Teilnahme',\n",
       " 'arbeiten']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = \"CDU\"\n",
    "meta_cluster_words[meta][meta_word_cluster[meta][\"arbeiten\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ungleichheit',\n",
       " 'Ungerechtigkeit',\n",
       " 'Ungleichheiten',\n",
       " 'Kälte',\n",
       " 'Spaltung',\n",
       " 'Ausgrenzung',\n",
       " 'Schieflage',\n",
       " 'Härten',\n",
       " 'Spannungen',\n",
       " 'Verwerfungen',\n",
       " 'Wohnraumförderung',\n",
       " 'Sicherungssysteme',\n",
       " 'Ungerechtigkeiten',\n",
       " 'Sicherungssystemen',\n",
       " 'Gerechtigkeit',\n",
       " 'Armut',\n",
       " 'Massenarbeitslosigkeit',\n",
       " 'Kluft',\n",
       " 'Marktwirtschaft',\n",
       " 'Kinderarmut',\n",
       " 'Komponente',\n",
       " 'Altersarmut',\n",
       " 'Reichtum',\n",
       " 'Schere',\n",
       " 'Minderausgabe']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = \"LINKE\"\n",
    "meta_cluster_words[meta][meta_word_cluster[meta][\"Reichtum\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eigenkapital',\n",
       " 'Geld',\n",
       " 'Kapital',\n",
       " 'Gelder',\n",
       " 'Investitionen',\n",
       " 'Kredite',\n",
       " 'Mittel',\n",
       " 'Personal',\n",
       " 'Einkommen',\n",
       " 'Aufträge',\n",
       " 'Vermögen',\n",
       " 'Wachstum']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = \"FDP\"\n",
    "meta_cluster_words[meta][meta_word_cluster[meta][\"Geld\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate the thematic centroid that helps us to identify clusters that are relevant to our discourse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The seed word schuften is not contained in model 1990!\n",
      "The seed word geschuftet is not contained in model 1990!\n",
      "The seed word schuften is not contained in model 2000!\n",
      "The seed word geschuftet is not contained in model 2000!\n",
      "The seed word geschuftet is not contained in model 2010!\n",
      "The seed word schuften is not contained in model CDU!\n",
      "The seed word geschuftet is not contained in model CDU!\n",
      "The seed word geschuftet is not contained in model SPD!\n",
      "The seed word schuften is not contained in model FDP!\n",
      "The seed word geschuftet is not contained in model FDP!\n",
      "The seed word schuften is not contained in model GRUENE!\n",
      "The seed word geschuftet is not contained in model GRUENE!\n",
      "The seed word geschuftet is not contained in model LINKE!\n"
     ]
    }
   ],
   "source": [
    "from lib import get_t_centroid_clusters\n",
    "\n",
    "meta_t_clusters = {}\n",
    "meta_t_centroid = {}\n",
    "meta_t_df = {}\n",
    "\n",
    "for meta in META_PATH_WEM.keys():\n",
    "    meta_t_clusters[meta], meta_t_centroid[meta], meta_t_df[meta] = (\n",
    "        get_t_centroid_clusters(\n",
    "            meta,\n",
    "            meta_model,\n",
    "            meta_word_cluster,\n",
    "            meta_cluster_words,\n",
    "            meta_cluster_centroid,\n",
    "            meta_cluster_label,\n",
    "            T_CENTROID_PAR[\"seed_words\"],\n",
    "            T_CENTROID_PAR[\"t_sim\"],\n",
    "            T_CENTROID_PAR[\"neighbors\"],\n",
    "            T_CENTROID_PAR[\"cutoff\"],\n",
    "            ANNOTATION_PAR[\"shorthand\"],\n",
    "            T_CENTROID_PAR[\"write_to_disc\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the clusters of the SPD subcorpus that are closest and most distant to the thematic centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cluster</th>\n",
       "      <th>n_words</th>\n",
       "      <th>thematic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>SPD_Lebenschancen|Zukunftschancen|Bildungschancen</td>\n",
       "      <td>320</td>\n",
       "      <td>20</td>\n",
       "      <td>0.638956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>SPD_Finanzmittel|Mittel|Haushaltsmittel</td>\n",
       "      <td>102</td>\n",
       "      <td>15</td>\n",
       "      <td>0.548024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>SPD_Chance|Möglichkeit|Chancen</td>\n",
       "      <td>438</td>\n",
       "      <td>4</td>\n",
       "      <td>0.547225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>SPD_gewährleisten|garantieren|sichern</td>\n",
       "      <td>613</td>\n",
       "      <td>12</td>\n",
       "      <td>0.544840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SPD_Kompetenzen|Kompetenz|Aufgaben</td>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>0.518926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>SPD_Jetzt|Nun|Dann</td>\n",
       "      <td>191</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.233925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>SPD_Diesen|Den</td>\n",
       "      <td>284</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.236208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>SPD_Als</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.241145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>SPD_Jedenfalls|Zumindest|Letztendlich</td>\n",
       "      <td>243</td>\n",
       "      <td>144</td>\n",
       "      <td>-0.264882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>SPD_Dieser|Der</td>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.269351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>983 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 label  cluster  n_words  \\\n",
       "320  SPD_Lebenschancen|Zukunftschancen|Bildungschancen      320       20   \n",
       "102            SPD_Finanzmittel|Mittel|Haushaltsmittel      102       15   \n",
       "438                     SPD_Chance|Möglichkeit|Chancen      438        4   \n",
       "613              SPD_gewährleisten|garantieren|sichern      613       12   \n",
       "54                  SPD_Kompetenzen|Kompetenz|Aufgaben       54        8   \n",
       "..                                                 ...      ...      ...   \n",
       "191                                 SPD_Jetzt|Nun|Dann      191        5   \n",
       "284                                     SPD_Diesen|Den      284        2   \n",
       "512                                            SPD_Als      512        1   \n",
       "243              SPD_Jedenfalls|Zumindest|Letztendlich      243      144   \n",
       "500                                     SPD_Dieser|Der      500        2   \n",
       "\n",
       "     thematic_similarity  \n",
       "320             0.638956  \n",
       "102             0.548024  \n",
       "438             0.547225  \n",
       "613             0.544840  \n",
       "54              0.518926  \n",
       "..                   ...  \n",
       "191            -0.233925  \n",
       "284            -0.236208  \n",
       "512            -0.241145  \n",
       "243            -0.264882  \n",
       "500            -0.269351  \n",
       "\n",
       "[983 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_t_df[\"SPD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in comparison, the clusters of the FDP subcorpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cluster</th>\n",
       "      <th>n_words</th>\n",
       "      <th>thematic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>FDP_investieren|sparen|ausgeben</td>\n",
       "      <td>273</td>\n",
       "      <td>19</td>\n",
       "      <td>0.597316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>FDP_realisieren|verwirklichen|organisieren</td>\n",
       "      <td>98</td>\n",
       "      <td>55</td>\n",
       "      <td>0.588040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>FDP_mitreden|aushalten|überleben</td>\n",
       "      <td>76</td>\n",
       "      <td>96</td>\n",
       "      <td>0.551479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>FDP_eingeleitet|durchgeführt|geleistet</td>\n",
       "      <td>209</td>\n",
       "      <td>29</td>\n",
       "      <td>0.548262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>FDP_stärken|sichern|fördern</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>0.547577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FDP_worden|wurde</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.228741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>FDP_hatte|hat|habe</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.263587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>FDP_Dieser|Der|Diesen</td>\n",
       "      <td>292</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.263767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>FDP_Immerhin|Jedenfalls|Letztendlich</td>\n",
       "      <td>64</td>\n",
       "      <td>145</td>\n",
       "      <td>-0.278808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>FDP_müsse|solle|wolle</td>\n",
       "      <td>228</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.285829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>551 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          label  cluster  n_words  \\\n",
       "273             FDP_investieren|sparen|ausgeben      273       19   \n",
       "98   FDP_realisieren|verwirklichen|organisieren       98       55   \n",
       "76             FDP_mitreden|aushalten|überleben       76       96   \n",
       "209      FDP_eingeleitet|durchgeführt|geleistet      209       29   \n",
       "57                  FDP_stärken|sichern|fördern       57       10   \n",
       "..                                          ...      ...      ...   \n",
       "21                             FDP_worden|wurde       21        2   \n",
       "85                           FDP_hatte|hat|habe       85        4   \n",
       "292                       FDP_Dieser|Der|Diesen      292        3   \n",
       "64         FDP_Immerhin|Jedenfalls|Letztendlich       64      145   \n",
       "228                       FDP_müsse|solle|wolle      228        7   \n",
       "\n",
       "     thematic_similarity  \n",
       "273             0.597316  \n",
       "98              0.588040  \n",
       "76              0.551479  \n",
       "209             0.548262  \n",
       "57              0.547577  \n",
       "..                   ...  \n",
       "21             -0.228741  \n",
       "85             -0.263587  \n",
       "292            -0.263767  \n",
       "64             -0.278808  \n",
       "228            -0.285829  \n",
       "\n",
       "[551 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_t_df[\"FDP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can annotate every token of our CWB corpus with the generated clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CORPUS_VRT_PATH, \"r\") as f:\n",
    "    with open(CORPUS_VRT_PATH.replace(\".vrt\", \"_clusters.vrt\"), \"w\") as f2:\n",
    "        s_attributes = {}\n",
    "        while line := f.readline():\n",
    "            if line.startswith(\"<\") and \" \" in line:\n",
    "                attribute = line.strip(\"<>\\n\").split(\" \")[0]\n",
    "                value = \" \".join(line.strip(\"<>\\n\").split(\" \")[1:])\n",
    "                s_attributes[attribute] = (\n",
    "                    value\n",
    "                    if value not in ANNOTATION_PAR[\"replace_meta\"]\n",
    "                    else ANNOTATION_PAR[\"replace_meta\"][value]\n",
    "                )\n",
    "\n",
    "            if \"\\t\" in line:\n",
    "                word = line.strip().split(\"\\t\")[ANNOTATION_PAR[\"column\"] - 1]\n",
    "                annotations = []\n",
    "                for s_attribute, metadata in ANNOTATION_PAR[\"s_attribute_meta\"].items():\n",
    "                    for meta in metadata:\n",
    "                        if meta == s_attributes[s_attribute]:\n",
    "                            if meta in ANNOTATION_PAR[\"ignore_meta\"]:\n",
    "                                annotations.append(\"-\")\n",
    "                                continue\n",
    "                            label = (\n",
    "                                meta_cluster_label[meta][meta_word_cluster[meta][word]]\n",
    "                                if word in meta_word_cluster[meta]\n",
    "                                else \"-\"\n",
    "                            )\n",
    "                            if label == \"-\":\n",
    "                                annotations.append(label)\n",
    "                                continue\n",
    "                            annotation = (\n",
    "                                ANNOTATION_PAR[\"shorthand\"][meta] + \"_\" + label\n",
    "                                if meta in ANNOTATION_PAR[\"shorthand\"]\n",
    "                                else meta + \"_\" + label\n",
    "                            )\n",
    "                            annotations.append(annotation)\n",
    "                        else:\n",
    "                            continue\n",
    "                if ANNOTATION_PAR[\"keep_layers\"]:\n",
    "                    f2.write(line.rstrip() + \"\\t\" + \"\\t\".join(annotations) + \"\\n\")\n",
    "                else:\n",
    "                    f2.write(word + \"\\t\" + \"\\t\".join(annotations) + \"\\n\")\n",
    "            else:\n",
    "                f2.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save a JSON file containing a mapping of the metadata/subcorpus to the contained clusters to the words contained in the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"output/meta_cluster_words.json\", \"w\") as f:\n",
    "    json.dump(meta_cluster_words, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdgrapher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
